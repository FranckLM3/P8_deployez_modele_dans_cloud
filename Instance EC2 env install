# connexion server 
ssh -i "EC2_P8_pyspark.pem" ubuntu@ec2-3-250-200-52.eu-west-1.compute.amazonaws.com

# Installation Spark Hadoop ect ..
sudo apt update 

#Java
sudo apt install openjdk-8-jre-headless


#If the instance does not have python install -> Anaconda
'''sudo apt install libgl1-mesa-glx libegl1-mesa libxrandr2 libxrandr2 libxss1 libxcursor1 libxcomposite1 libasound2 libxi6 libxtst6
wget -P /tmp https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh
sha256sum /tmp/Anaconda3-2020.02-Linux-x86_64.sh
bash /tmp/Anaconda3-2020.02-Linux-x86_64.sh
source ~/.profile'''

#Scala
sudo apt install scala

#Spark
wget https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz

tar xvf spark-*
sudo mv spark-3.2.1-bin-hadoop2.7 /opt/spark

echo "export SPARK_HOME=/opt/spark" >> ~/.profile
echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.profile
echo "export PYSPARK_PYTHON=/usr/bin/python3.10" >> ~/.profile
source ~/.profile

# AWS cli 
sudo apt install awscli
aws configure

sudo apt install python3-pip

pip install pandas tensorflow pillow findspark pyarrow boto3 fsspec s3fs seaborn

# Install SWAP taille swap 2x RAM 
https://shurn.me/blog/2017-02-13/swap-space-in-ec2-ubuntu

# Send file to EC2
scp -i "EC2_P8_pyspark.pem" /Users/franck/Documents/P8_deployez_modele_cloud/pysparkApp_cloud.py ubuntu@ec2-54-173-139-254.compute-1.amazonaws.com:~

